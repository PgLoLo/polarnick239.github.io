---
layout: ru/blogs/gpgpu/post
title:  "CUDA аллокации занимают до 600 ms"
date:   2017-05-13 21:30:00 +0300
categories: gpu cuda nvidia
lang:   ru
id:     3_cuda_mallocs
---

На двухсокетном компьютере с двумя Quadro P6000 GPU и Windows: алгоритм тормозил - утилизация видеокарт колебалось между 10% и 30%.
 При этом на том же двухсокетном комьютере под Linux проблема не наблюдалась.
 На односокетном компьютере с Windows тоже все хорошо: утилизация видеокарт около 90%.

Под профилировщиком оказалось, что аллокации на двухсокетной системе под Windows занимают до 600 миллисекунд (при этом занято лишь 23 из 24 Гб видеопамяти):

![Slow CUDA mallocs](/static/2017/05/01/slowCudaMallocs.png)

Так как алгоритм на самом деле обрабатывает большой объем данных по частям и производит аллокации для каждого куска,
 то можно обойти проблему переиспользуя аллоцированные ранее куски памяти. Но это усложняет код, увеличивает шансы на утечку памяти и вообще на дворе 2к17.  

Судя по всему проблема в WDDM 2.x которая проявляется на некоторых NUMA-архитектурах. Чтобы использовать NVIDIA видеокарту не в WDDM режиме - ее можно перевести в TCC режим.
 Но этот режим поддерживают только Quadro/Tesla/Titan видеокарты (**кроме Titan Xp** - [devtalks](https://devtalk.nvidia.com/default/topic/1007197/tcc-support-for-titan-xp-not-yet-implemented-/)). И только те, которые не используются как видеоадаптеры, но что делать, если у вас сервер у которого всего два
 слота под видеокарты, и одна из них подключена к дисплею? Либо избавляться от частых аллокаций, либо использовать только вторую видеокарту в TCC-режиме.

Обсуждение на [devtalks](https://devtalk.nvidia.com/default/topic/924453/-multiple-gpus-processes-cuda-memory-de-allocation-slow).

Результаты профилирования при одной видеокарте в WDDM режиме и второй - в TCC:

![Fixed CUDA mallocs](/static/2017/05/01/fixedCudaMallocs.png)
